{
    "contents" : "\\documentclass{article}\n\\usepackage[margin=1.25in]{geometry}\n\\usepackage{graphicx, hyperref, float, multicol, pdflscape, enumerate, paralist}\n\\usepackage{amssymb,amsmath,amsthm} \n\\usepackage[backend=bibtex, natbib=true]{biblatex}\n\\addbibresource{../references/references.bib}\n\n\\usepackage{color}\n\\newcommand{\\ak}[1]{{\\color{magenta} #1}}\n\\newcommand{\\mj}[1]{{\\color{blue} #1}}\n\n\\theoremstyle{plain}\n\\newtheorem{res}{Result}\n\n\\setlength{\\parindent}{0cm}\n\\renewcommand{\\baselinestretch}{1.5}\n\n\\title{Extensive Assignment 1 \\\\ {STAT 601}}\n\\author{Andee Kaplan \\& Maggie Johnson}\n\\date{February 28, 2014}\n\n\\begin{document}\n\n\\maketitle\n<<load-data, echo=FALSE, warning=FALSE, error=FALSE, cache=TRUE>>=\nlibrary(lubridate)\n\ndat <- read.table(\"data/greenbeandat.txt\", header=TRUE)\ndat$date <-  ymd(as.character(dat$date))\n@\n\n\\section*{Model}\nLet $\\{Y_{ij}: i=1,\\dots,n_j, j=1,\\dots,m\\}$ represent the quantity of green beans sold on day $i$ at store $j$, let $\\{Z_{ij}: i=1,\\dots,n_j, j=1,\\dots,m\\}$ represent the unobservable construct of consumer interest on day $i$ at store $j$, and let $\\{x_{ij}: i=1,\\dots,n_j, j=1,\\dots,m\\}$ be the price of green beans on day $i$ at store $j$. There are $m=\\Sexpr{length(unique(dat$store))}$ stores in the midwest region. Then, we impose the following model.\n\\begin{align*}\nZ_{ij} &\\stackrel{\\text{iid}}{\\sim} \\text{Bern}(p_{ij}) \\\\\nY_{ij} | Z_{ij} = 1 &\\stackrel{\\text{indep}}{\\sim} \\text{Pois}(\\lambda_{ij}) \\\\\nPr(Y_{ij} =0| Z_{ij} = 0) &= 1\n\\end{align*}\n\nThis yields the following marginal distribution of $Y_{ij}$ for $\\lambda_{ij} > 0$ and $0 < p_{ij} < 1$:\n\\begin{align*}\nf(y_{ij}|p_{ij}, \\lambda_{ij}) &= \\left[(1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij})\\right] \\mathbb{I}\\{y_{ij} = 0\\} + \\left[ \\frac{p_{ij}}{y_{ij} !} \\lambda_{ij}^{y_{ij}}\\exp(-\\lambda_{ij}) \\right]\\mathbb{I}\\{y_{ij} > 0\\} \\\\\n&= \\left[(1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij})\\right]^{\\delta_{ij}} \\left[ \\frac{p_{ij}}{y_{ij} !} \\lambda_{ij}^{y_{ij}}\\exp(-\\lambda_{ij}) \\right] ^{1-\\delta_{ij}}\n\\end{align*}\nwhere \n\\begin{align*}\n\\delta_{ij} = \\begin{cases}\n1 & y_{ij} = 0 \\\\\n0 & y_{ij} > 0\n\\end{cases}\n\\end{align*}\n\n\\subsection*{Systematic Components}\nWe would like to have both the parameters $p_{ij}$ from the Bernoulli distribution and the parameters $\\lambda_{ij}$ from the Poisson portion of the model to be further modeled as a function of price ($x_{ij}$). So, we will use the constructs of GLM to model the expected values of $Y_{ij}$ and $Z_{ij}$ as inverse link functions of the simple regression equations.\n\\begin{align*}\n\\text{E}(Z_{ij}) &= p_{ij} \\\\\n% \\text{E}(Y_{ij}) &= \\sum\\limits_{y_{ij} = 1}^\\infty p_{ij} y_{ij} \\lambda_{ij}^{y_{ij}} \\exp(-\\lambda_{ij}) \\\\\n% &= \\sum\\limits_{y_{ij} = 0}^\\infty p_{ij} y_{ij} \\lambda_{ij}^{y_{ij}} \\exp(-\\lambda_{ij}) \\\\\n% &= p_{ij} \\lambda_{ij}\n\\text{E}(Y_{ij}|Z_{ij}) &= \\lambda_{ij}\n\\end{align*}\nWe will use a logit-link for $p_{ij}$ and a log-link for $\\lambda_{ij}$, which yields the following values.\n\\begin{align*}\n\\log\\left( \\frac{p_{ij}}{1-p_{ij}}\\right) &= \\beta_0 + \\beta_1 x_{ij} \\\\ \n\\Rightarrow p_{ij} &= \\frac{\\exp(\\beta_0 + \\beta_1 x_{ij})}{1 + \\exp(\\beta_0 + \\beta_1 x_{ij})} \\\\\n~\\\\\n\\log(\\lambda_{ij}) &= \\beta_2 + \\beta_3 x_{ij} \\\\\n\\Rightarrow\\lambda_{ij} &= \\exp(\\beta_2 + \\beta_3 x_{ij}) \\\\\n% \\Rightarrow \\lambda_{ij} &= \\frac{ \\exp(\\beta_2 + \\beta_3 x_{ij})}{ \\exp(\\beta_0 + \\beta_1 x_{ij})} (1 +  \\exp(\\beta_0 + \\beta_1 x_{ij}))\n\\end{align*}\n\\subsection*{Likelihood}\nTo obtain the store likelihood $L_j(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4)$, first we will write the joint density for store $j$.\n% \\begin{align*}\n% L_j(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4) &= f(y_{1j}, \\dots, y_{{n_j},j} | \\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4) \\\\\n% &= \\prod\\limits_{i=1}^{n_j} f(y_{ij}|\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4) \\tag{\\text{independence}} \\\\\n% &= \\prod\\limits_{i=1}^{n_j} \\left(\\left[(1-p_{ij}(\\boldsymbol \\beta)) + p_{ij}(\\boldsymbol \\beta) \\exp(-\\lambda_{ij}(\\boldsymbol \\beta))\\right] \\mathbb{I}\\{y_{ij} = 0\\} + \\right. \\\\\n% & \\qquad \\qquad \\left. \\left[ \\frac{p_{ij}(\\boldsymbol \\beta)}{y_{ij} !} \\lambda_{ij}(\\boldsymbol \\beta)^{y_{ij}}\\exp(-\\lambda_{ij}(\\boldsymbol \\beta)) \\right]\\mathbb{I}\\{y_{ij} > 0\\} \\right)\n% \\end{align*}\n\\begin{align*}\nL_j(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4) &= f(y_{1j}, \\dots, y_{{n_j},j} | \\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4) \\\\\n&= \\prod\\limits_{i=1}^{n_j} f(y_{ij}|\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4) \\tag{\\text{independence}} \\\\\n&= \\prod\\limits_{i=1}^{n_j} \\left(\\left[(1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij})\\right]^{\\delta_{ij}} \\left[ \\frac{p_{ij}}{y_{ij} !} \\lambda_{ij}^{y_{ij}}\\exp(-\\lambda_{ij}) \\right] ^{1-\\delta_{ij}} \\right)\n\\end{align*}\nWhere $p_{ij}$ and $\\lambda_{ij}$ are functions of $x_ij$ and $\\beta_0, \\dots, \\beta_3$. Then the log-likelihood function can be written as the following.\n\\begin{align*}\nl_j(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4) &= \\sum\\limits_{i=1}^{n_j} \\left(\\delta_{ij} \\log\\left[(1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij})\\right] + (1-\\delta_{ij}) \\left[ \\log (p_{ij}) - \\log(y_{ij} !) + y_{ij}\\log(\\lambda_{ij})-\\lambda_{ij} \\right] \\right)\n\\end{align*}\nWe can now find the Jacobian and the Hessian matrix to facilitate finding the MLEs using a Newton-Raphson method:\n\\begin{align*}\n\\frac{\\partial l_j}{\\partial \\beta_k} &= \\frac{\\partial l_j}{\\partial p_{ij}}\\frac{\\partial p_{ij}}{\\partial \\beta_k} + \\frac{\\partial l_j}{\\partial \\lambda_{ij}}\\frac{\\partial \\lambda_{ij}}{\\partial \\beta_k} \\\\\n\\frac{\\partial^2 l_j}{\\partial \\beta_k \\partial \\beta_l} &= \\left[\\frac{\\partial^2 l_j}{\\partial p_{ij}^2}\\frac{\\partial p_{ij}}{\\partial \\beta_l} + \\frac{\\partial^2 l_j}{\\partial p_{ij}\\partial \\lambda_{ij}}\\frac{\\partial \\lambda_{ij}}{\\partial \\beta_l}\\right] \\frac{\\partial p_{ij}}{\\partial \\beta_k} + \\frac{\\partial^2 p_{ij}}{\\partial \\beta_k \\partial \\beta_l} \\frac{\\partial l_j}{\\partial p_{ij}} + \\\\\n& \\qquad \\qquad \\left[\\frac{\\partial^2 l_j}{\\partial \\lambda_{ij} \\partial p_{ij}}\\frac{\\partial p_{ij}}{\\partial \\beta_l} + \\frac{\\partial^2 l_j}{\\partial \\partial \\lambda_{ij}^2}\\frac{\\partial \\lambda_{ij}}{\\partial \\beta_l}\\right] \\frac{\\partial \\lambda_{ij}}{\\partial \\beta_k} + \\frac{\\partial^2 \\lambda_{ij}}{\\partial \\beta_k \\partial \\beta_l} \\frac{\\partial l_j}{\\partial \\lambda_{ij}}\n\\end{align*}\nfor $k,l=0,1,2,3$.\n\nNow let $T_1 = \\exp{\\beta_0 + \\beta_1 x_{ij}}$ and $T_2 = \\exp{\\beta_2 + \\beta_3 x_{ij}}$. Then, the partial derivatives can be computed as follows.\n\\begin{align*}\n\\frac{\\partial l_i}{\\partial p_{ij}} &= \\sum\\limits_{i=1}^{n_j}\\left\\{ \\delta_{ij} \\frac{\\exp(-\\lambda_{ij}) - 1}{(1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij})} + (1-\\delta_{ij}) \\frac{1}{p_{ij}} \\right\\}  \\\\\n\\frac{\\partial l_i}{\\partial \\lambda_{ij}} &= \\sum\\limits_{i=1}^{n_j}\\left\\{ \\delta_{ij} \\frac{-p_{ij}\\exp(-\\lambda_{ij})}{(1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij})} + (1-\\delta_{ij}) \\left[\\frac{y_{ij}}{\\lambda_{ij}} - 1\\right] \\right\\} \\\\\n\\frac{\\partial p_{ij}}{\\partial \\beta_0} &= \\frac{T_1}{(1+ T_1)^2} \\\\\n\\frac{\\partial p_{ij}}{\\partial \\beta_1} &= x_{ij} \\frac{T_1}{(1+ T_1)^2} \\\\\n\\frac{\\partial \\lambda_{ij}}{\\partial \\beta_2} &= T_2\\\\\n\\frac{\\partial \\lambda_{ij}}{\\partial \\beta_3} &= x_{ij} T_2\\\\\n\\frac{\\partial^2 l_j}{\\partial p_{ij}^2} &= \\sum\\limits_{i=1}^{n_j}\\left\\{ -\\delta_{ij} \\left(\\frac{\\exp(-\\lambda_{ij}) - 1}{(1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij})}\\right)^2 - (1-\\delta_{ij}) \\frac{1}{p_{ij}^2} \\right\\}\\\\\n\\frac{\\partial^2 l_j}{\\partial p_{ij} \\partial \\lambda_{ij}} &= \\sum\\limits_{i=1}^{n_j}\\left\\{ \\delta_{ij} \\left(\\frac{-(\\exp(-\\lambda_{ij}) - 1)\\exp(-\\lambda_{ij})}{(1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij})} - \\frac{(\\exp(-\\lambda_{ij}) - 1)p_{ij}\\exp(-\\lambda_{ij})}{((1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij}))^2} \\right)\\right\\}\\\\\n\\frac{\\partial^2 l_j}{\\partial \\lambda_{ij}^2} &= \\sum\\limits_{i=1}^{n_j}\\left\\{ \\delta_{ij} \\left[\\frac{p_{ij}\\exp(-\\lambda_{ij})}{(1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij})} -\\left(\\frac{p_{ij}\\exp(-\\lambda_{ij})}{(1-p_{ij}) + p_{ij} \\exp(-\\lambda_{ij})}\\right)^2\\right] - (1-\\delta_{ij}) \\left[\\frac{y_{ij}}{\\lambda_{ij}^2}\\right] \\right\\}\\\\\n\\frac{\\partial^2 p_{ij}}{\\partial \\beta_0 \\partial \\beta_0} &= \\left( \\frac{T_1}{1 + T_1}\\right)^2 - 2\\left(\\frac{T_1}{1 + T_1}\\right)^3\\\\\n\\frac{\\partial^2 p_{ij}}{\\partial \\beta_0 \\partial \\beta_1} &= x_{ij}\\left[ \\left( \\frac{T_1}{1 + T_1}\\right)^2 - 2\\left(\\frac{T_1}{1 + T_1}\\right)^3 \\right]\\\\\n\\frac{\\partial^2 p_{ij}}{\\partial \\beta_1 \\partial \\beta_1} &= x_{ij}^2\\left[ \\left( \\frac{T_1}{1 + T_1}\\right)^2 - 2\\left(\\frac{T_1}{1 + T_1}\\right)^3 \\right]\\\\\n\\frac{\\partial^2 \\lambda_{ij}}{\\partial \\beta_2 \\partial \\beta_2} &= T_2\\\\\n\\frac{\\partial^2 \\lambda_{ij}}{\\partial \\beta_2 \\partial \\beta_3} &= x_{ij}T_2\\\\\n\\frac{\\partial^2 \\lambda_{ij}}{\\partial \\beta_3 \\partial \\beta_3} &= x_{ij}^2 T_2\n\\end{align*}\nWhere the remaining partial derivatives are zero.\n\\end{document}",
    "created" : 1392341805770.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "464066494",
    "id" : "F8516819",
    "lastKnownWriteTime" : 1392509730,
    "path" : "~/GitHub/ExtensiveAssignments/EA 1/Kaplan_Johnson_EA1.Rnw",
    "project_path" : "EA 1/Kaplan_Johnson_EA1.Rnw",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "sweave"
}