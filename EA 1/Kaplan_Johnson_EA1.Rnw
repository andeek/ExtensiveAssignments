\documentclass{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx, hyperref, float, multicol, pdflscape, enumerate, paralist}
\usepackage{amssymb,amsmath,amsthm} 
\usepackage[backend=bibtex, natbib=true]{biblatex}
\addbibresource{../references/references.bib}

\usepackage{color}
\newcommand{\ak}[1]{{\color{magenta} #1}}
\newcommand{\mj}[1]{{\color{blue} #1}}

\theoremstyle{plain}
\newtheorem{res}{Result}

\setlength{\parindent}{0cm}
\renewcommand{\baselinestretch}{1.5}

\title{Extensive Assignment 1 \\ {STAT 601}}
\author{Andee Kaplan \& Maggie Johnson}
\date{February 28, 2014}

\begin{document}

\maketitle
<<load-data, echo=FALSE, warning=FALSE, error=FALSE, cache=TRUE>>=
library(lubridate)

dat <- read.table("data/greenbeandat.txt", header=TRUE)
dat$date <-  ymd(as.character(dat$date))
@

\section*{Model}
Let $\{Y_{ij}: i=1,\dots,n_j, j=1,\dots,m\}$ represent the quantity of green beans sold on day $i$ at store $j$, let $\{Z_{ij}: i=1,\dots,n_j, j=1,\dots,m\}$ represent the unobservable construct of consumer interest on day $i$ at store $j$, and let $\{x_{ij}: i=1,\dots,n_j, j=1,\dots,m\}$ be the price of green beans on day $i$ at store $j$. There are $m=\Sexpr{length(unique(dat$store))}$ stores in the midwest region. Then, we impose the following model.
\begin{align*}
Z_{ij} &\stackrel{\text{iid}}{\sim} \text{Bern}(p_{ij}) \\
Y_{ij} | Z_{ij} = 1 &\stackrel{\text{indep}}{\sim} \text{Pois}(\lambda_{ij}) \\
Pr(Y_{ij} =0| Z_{ij} = 0) &= 1
\end{align*}

This yields the following marginal distribution of $Y_{ij}$ for $\lambda_{ij} > 0$ and $0 < p_{ij} < 1$:
\begin{align*}
f(y_{ij}|p_{ij}, \lambda_{ij}) &= \left[(1-p_{ij}) + p_{ij} \exp(-\lambda_{ij})\right] \mathbb{I}\{y_{ij} = 0\} + \left[ \frac{p_{ij}}{y_{ij} !} \lambda_{ij}^{y_{ij}}\exp(-\lambda_{ij}) \right]\mathbb{I}\{y_{ij} > 0\} \\
&= \left[(1-p_{ij}) + p_{ij} \exp(-\lambda_{ij})\right]^{\delta_{ij}} \left[ \frac{p_{ij}}{y_{ij} !} \lambda_{ij}^{y_{ij}}\exp(-\lambda_{ij}) \right] ^{1-\delta_{ij}}
\end{align*}
where 
\begin{align*}
\delta_{ij} = \begin{cases}
1 & y_{ij} = 0 \\
0 & y_{ij} > 0
\end{cases}
\end{align*}

\subsection*{Systematic Components}
We would like to have both the parameters $p_{ij}$ from the Bernoulli distribution and the parameters $\lambda_{ij}$ from the Poisson portion of the model to be further modeled as a function of price ($x_{ij}$). So, we will use the constructs of GLM to model the expected values of $Y_{ij}$ and $Z_{ij}$ as inverse link functions of the simple regression equations.
\begin{align*}
\text{E}(Z_{ij}) &= p_{ij} \\
% \text{E}(Y_{ij}) &= \sum\limits_{y_{ij} = 1}^\infty p_{ij} y_{ij} \lambda_{ij}^{y_{ij}} \exp(-\lambda_{ij}) \\
% &= \sum\limits_{y_{ij} = 0}^\infty p_{ij} y_{ij} \lambda_{ij}^{y_{ij}} \exp(-\lambda_{ij}) \\
% &= p_{ij} \lambda_{ij}
\text{E}(Y_{ij}|Z_{ij}) &= \lambda_{ij}
\end{align*}
We will use a logit-link for $p_{ij}$ and a log-link for $\lambda_{ij}$, which yields the following values.
\begin{align*}
\log\left( \frac{p_{ij}}{1-p_{ij}}\right) &= \beta_0 + \beta_1 x_{ij} \\ 
\Rightarrow p_{ij} &= \frac{\exp(\beta_0 + \beta_1 x_{ij})}{1 + \exp(\beta_0 + \beta_1 x_{ij})} \\
~\\
\log(\lambda_{ij}) &= \beta_2 + \beta_3 x_{ij} \\
\Rightarrow\lambda_{ij} &= \exp(\beta_2 + \beta_3 x_{ij}) \\
% \Rightarrow \lambda_{ij} &= \frac{ \exp(\beta_2 + \beta_3 x_{ij})}{ \exp(\beta_0 + \beta_1 x_{ij})} (1 +  \exp(\beta_0 + \beta_1 x_{ij}))
\end{align*}
\subsection*{Likelihood}
To obtain the store likelihood $L_j(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4)$, first we will write the joint density for store $j$.
% \begin{align*}
% L_j(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4) &= f(y_{1j}, \dots, y_{{n_j},j} | \beta_0, \beta_1, \beta_2, \beta_3, \beta_4) \\
% &= \prod\limits_{i=1}^{n_j} f(y_{ij}|\beta_0, \beta_1, \beta_2, \beta_3, \beta_4) \tag{\text{independence}} \\
% &= \prod\limits_{i=1}^{n_j} \left(\left[(1-p_{ij}(\boldsymbol \beta)) + p_{ij}(\boldsymbol \beta) \exp(-\lambda_{ij}(\boldsymbol \beta))\right] \mathbb{I}\{y_{ij} = 0\} + \right. \\
% & \qquad \qquad \left. \left[ \frac{p_{ij}(\boldsymbol \beta)}{y_{ij} !} \lambda_{ij}(\boldsymbol \beta)^{y_{ij}}\exp(-\lambda_{ij}(\boldsymbol \beta)) \right]\mathbb{I}\{y_{ij} > 0\} \right)
% \end{align*}
\begin{align*}
L_j(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4) &= f(y_{1j}, \dots, y_{{n_j},j} | \beta_0, \beta_1, \beta_2, \beta_3, \beta_4) \\
&= \prod\limits_{i=1}^{n_j} f(y_{ij}|\beta_0, \beta_1, \beta_2, \beta_3, \beta_4) \tag{\text{independence}} \\
&= \prod\limits_{i=1}^{n_j} \left(\left[(1-p_{ij}) + p_{ij} \exp(-\lambda_{ij})\right]^{\delta_{ij}} \left[ \frac{p_{ij}}{y_{ij} !} \lambda_{ij}^{y_{ij}}\exp(-\lambda_{ij}) \right] ^{1-\delta_{ij}} \right)
\end{align*}
Where $p_{ij}$ and $\lambda_{ij}$ are functions of $x_ij$ and $\beta_0, \dots, \beta_3$. Then the log-likelihood function can be written as the following.
\begin{align*}
l_j(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4) &= \sum\limits_{i=1}^{n_j} \left(\delta_{ij} \log\left[(1-p_{ij}) + p_{ij} \exp(-\lambda_{ij})\right] + (1-\delta_{ij}) \left[ \log (p_{ij}) - \log(y_{ij} !) + y_{ij}\log(\lambda_{ij})-\lambda_{ij} \right] \right)
\end{align*}
We can now find the Jacobian and the Hessian matrix to facilitate finding the MLEs using a Newton-Raphson method:
\begin{align*}
\frac{\partial l_j}{\partial \beta_k} &= \frac{\partial l_j}{\partial p_{ij}}\frac{\partial p_{ij}}{\partial \beta_k} + \frac{\partial l_j}{\partial \lambda_{ij}}\frac{\partial \lambda_{ij}}{\partial \beta_k} \\
\frac{\partial^2 l_j}{\partial \beta_k \partial \beta_l} &= \left[\frac{\partial^2 l_j}{\partial p_{ij}^2}\frac{\partial p_{ij}}{\partial \beta_l} + \frac{\partial^2 l_j}{\partial p_{ij}\partial \lambda_{ij}}\frac{\partial \lambda_{ij}}{\partial \beta_l}\right] \frac{\partial p_{ij}}{\partial \beta_k} + \frac{\partial^2 p_{ij}}{\partial \beta_k \partial \beta_l} \frac{\partial l_j}{\partial p_{ij}} + \\
& \qquad \qquad \left[\frac{\partial^2 l_j}{\partial \lambda_{ij} \partial p_{ij}}\frac{\partial p_{ij}}{\partial \beta_l} + \frac{\partial^2 l_j}{\partial \partial \lambda_{ij}^2}\frac{\partial \lambda_{ij}}{\partial \beta_l}\right] \frac{\partial \lambda_{ij}}{\partial \beta_k} + \frac{\partial^2 \lambda_{ij}}{\partial \beta_k \partial \beta_l} \frac{\partial l_j}{\partial \lambda_{ij}}
\end{align*}
for $k,l=0,1,2,3$.

Now let $T_1 = \exp{\beta_0 + \beta_1 x_{ij}}$ and $T_2 = \exp{\beta_2 + \beta_3 x_{ij}}$. Then, the partial derivatives can be computed as follows.
\begin{align*}
\frac{\partial l_i}{\partial p_{ij}} &= \sum\limits_{i=1}^{n_j}\left\{ \delta_{ij} \frac{\exp(-\lambda_{ij}) - 1}{(1-p_{ij}) + p_{ij} \exp(-\lambda_{ij})} + (1-\delta_{ij}) \frac{1}{p_{ij}} \right\}  \\
\frac{\partial l_i}{\partial \lambda_{ij}} &= \sum\limits_{i=1}^{n_j}\left\{ \delta_{ij} \frac{-p_{ij}\exp(-\lambda_{ij})}{(1-p_{ij}) + p_{ij} \exp(-\lambda_{ij})} + (1-\delta_{ij}) \left[\frac{y_{ij}}{\lambda_{ij}} - 1\right] \right\} \\
\frac{\partial p_{ij}}{\partial \beta_0} &= \frac{T_1}{(1+ T_1)^2} \\
\frac{\partial p_{ij}}{\partial \beta_1} &= x_{ij} \frac{T_1}{(1+ T_1)^2} \\
\frac{\partial \lambda_{ij}}{\partial \beta_2} &= T_2\\
\frac{\partial \lambda_{ij}}{\partial \beta_3} &= x_{ij} T_2\\
\frac{\partial^2 l_j}{\partial p_{ij}^2} &= \sum\limits_{i=1}^{n_j}\left\{ -\delta_{ij} \left(\frac{\exp(-\lambda_{ij}) - 1}{(1-p_{ij}) + p_{ij} \exp(-\lambda_{ij})}\right)^2 - (1-\delta_{ij}) \frac{1}{p_{ij}^2} \right\}\\
\frac{\partial^2 l_j}{\partial p_{ij} \partial \lambda_{ij}} &= \sum\limits_{i=1}^{n_j}\left\{ \delta_{ij} \left(\frac{-(\exp(-\lambda_{ij}) - 1)\exp(-\lambda_{ij})}{(1-p_{ij}) + p_{ij} \exp(-\lambda_{ij})} - \frac{(\exp(-\lambda_{ij}) - 1)p_{ij}\exp(-\lambda_{ij})}{((1-p_{ij}) + p_{ij} \exp(-\lambda_{ij}))^2} \right)\right\}\\
\frac{\partial^2 l_j}{\partial \lambda_{ij}^2} &= \sum\limits_{i=1}^{n_j}\left\{ \delta_{ij} \left[\frac{p_{ij}\exp(-\lambda_{ij})}{(1-p_{ij}) + p_{ij} \exp(-\lambda_{ij})} -\left(\frac{p_{ij}\exp(-\lambda_{ij})}{(1-p_{ij}) + p_{ij} \exp(-\lambda_{ij})}\right)^2\right] - (1-\delta_{ij}) \left[\frac{y_{ij}}{\lambda_{ij}^2}\right] \right\}\\
\frac{\partial^2 p_{ij}}{\partial \beta_0 \partial \beta_0} &= \left( \frac{T_1}{1 + T_1}\right)^2 - 2\left(\frac{T_1}{1 + T_1}\right)^3\\
\frac{\partial^2 p_{ij}}{\partial \beta_0 \partial \beta_1} &= x_{ij}\left[ \left( \frac{T_1}{1 + T_1}\right)^2 - 2\left(\frac{T_1}{1 + T_1}\right)^3 \right]\\
\frac{\partial^2 p_{ij}}{\partial \beta_1 \partial \beta_1} &= x_{ij}^2\left[ \left( \frac{T_1}{1 + T_1}\right)^2 - 2\left(\frac{T_1}{1 + T_1}\right)^3 \right]\\
\frac{\partial^2 \lambda_{ij}}{\partial \beta_2 \partial \beta_2} &= T_2\\
\frac{\partial^2 \lambda_{ij}}{\partial \beta_2 \partial \beta_3} &= x_{ij}T_2\\
\frac{\partial^2 \lambda_{ij}}{\partial \beta_3 \partial \beta_3} &= x_{ij}^2 T_2
\end{align*}
Where the remaining partial derivatives are zero.
\end{document}