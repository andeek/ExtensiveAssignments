An alternative to the homogeneous model would be to assume that the two different steel types have different rates of flaws. We consider a mixture poisson model with the same observable random variables as for the homogeneous model, and the following likelihood function 
\begin{align*}
h(y_i|\gamma_i, \lambda_1,\lambda_2,\phi) &= \phi f_1(y_i|\gamma_i, \lambda_1) + (1-\phi)f_2(y_i|\gamma_i, \lambda_2) \\
&= \phi \frac{(\gamma_i \lambda_1)^{y_i}}{y_i!}e^{-\gamma_i \lambda_1} + (1-\phi) \frac{(\gamma_i \lambda_2)^{y_i}}{y_i!}e^{-\gamma_i \lambda_2}.
\end{align*}
where $\lambda_1, \lambda_2 \in (0,\infty)$, $0 < \phi < 1$, and $\gamma_i$ is again the length of track segment $i$. $\lambda_1$ and $\lambda_2$ represent the number of flaws per km for tracks of type A and type B, respectively. $\phi$ represents the overall probability that a section of track is laid with steel type A. Estimation of this model could be achieved through direct maximization of the likelihood, but we will instead use the EM algorithm and formulate the model as a missing information problem. The EM algorithm allows for  estimation of $\lambda_1$, $\lambda_2$, and $\phi$, as well as simple estimation of the probability that each individual segment ($i$) is constructed of steel type A. 

Define the unobserved random variables $\{Z_{i}\}$ such that 
\begin{align*}
Z_{i} = \begin{cases}
1 & \text{if } Y_i \text{ is from steel type A}\\
0 & \text{if } Y_i \text{ is from steel type B}\\
\end{cases}
\end{align*}
The probability mass function of $Z_{i}$ is then
\begin{align*}
g(z_i|\phi) = \phi^{z_{i}}(1-\phi)^{1-z_{i}}
\end{align*}
where $z_i \in \{0,1\}$. Then conditional on $Z_i = z_i$, $Y_i$ has density function
\begin{align*}
f_i(y_i|z_i, \lambda_1,\lambda_2) &= f_1(y_i|z_i, \lambda_1)^{z_{i}}f_2(y_i|z_i, \lambda_2)^{(1-z_{i})} \\
&= \left(\frac{(\gamma_i \lambda_1)^{y_i}}{y_i!}e^{-\gamma_i \lambda_1}\right)^{z_{i}}\left(\frac{(\gamma_i \lambda_2)^{y_i}}{y_i!}e^{-\gamma_i \lambda_2}\right)^{(1-z_{i})}
\end{align*}
and $Y_i$ and $Z_i$ have joint mixed density function
\begin{align*}
p_i(y_i,z_i|\lambda_1,\lambda_2,\phi) &= f_i(y_i|z_i, \lambda_1, \lambda_2)g(z_i|\phi) \\
&= \left(\phi \frac{(\gamma_i \lambda_1)^{y_i}}{y_i!}e^{-\gamma_i \lambda_1}\right)^{z_{i}}\left((1-\phi) \frac{(\gamma_i \lambda_2)^{y_i}}{y_i!}e^{-\gamma_i \lambda_2}\right)^{(1-z_{i})}
\end{align*}
The conditional mass function for the unobserved $Z_i$ is,
\begin{align*}
k_i(z_i|y_i,\lambda_1, \lambda_2,\phi)
&= \frac{\left(\phi \frac{(\gamma_i \lambda_1)^{y_i}}{y_i!}e^{-\gamma_i \lambda_1}\right)^{z_{i}}\left((1-\phi) \frac{(\gamma_i \lambda_2)^{y_i}}{y_i!}e^{-\gamma_i \lambda_2}\right)^{(1-z_{i})}}{\phi \frac{(\gamma_i \lambda_1)^{y_i}}{y_i!}e^{-\gamma_i \lambda_1} + (1-\phi) \frac{(\gamma_i \lambda_2)^{y_i}}{y_i!}e^{-\gamma_i \lambda_2}}
\end{align*}
We can now form the full observed data model, unobserved marginal model, complete data model, and conditional unoberved model, by independence:
\begin{align*}
f(\boldsymbol y | \boldsymbol z, \lambda_1, \lambda_2,\phi) &= \prod\limits_{i=1}^n f_i(y_i | z_i, \lambda_1, \lambda_2,\phi) \\
g(\boldsymbol z | \phi) &= \prod\limits_{i=1}^n g(z_i |\phi) \\
p(\boldsymbol y , \boldsymbol z | \lambda_1, \lambda_2,\phi) &= \prod\limits_{i=1}^n p_i(y_i, z_i| \lambda_1, \lambda_2,\phi) \\
k(\boldsymbol z | \boldsymbol y, \lambda_1, \lambda_2,\phi) &= \prod\limits_{i=1}^n k_i(z_i | y_i, \lambda_1, \lambda_2,\phi) \\
\end{align*}
Thus, we can write the functons $L$, $Q$, and $H$ in our formulation of the EM algorthm as sums over $i$. Let $\boldsymbol \theta = (\lambda_1, \lambda_2)$, then
\begin{align*}
Q(\boldsymbol \theta, \phi|\boldsymbol \theta_p, \phi_p) &= \sum\limits_{i=1}^n \text{E}_{z|y}[\log\{p_i(y_i, z_i|\boldsymbol \theta, \phi)\}|\boldsymbol \theta_p, \phi_p] \\
&= \sum\limits_{i=1}^n \text{E}_{z|y}[\log\{f_i(y_i| z_i,\boldsymbol \theta)\}|\boldsymbol \theta_p, \phi_p] + \sum\limits_{i=1}^n \text{E}_{z|y}[\log\{g(z_i| \phi)\}|\boldsymbol \theta_p, \phi_p] 
\end{align*}
And substituting the expression for $f_i$ in this expression yields,
\begin{align*}
\text{E}_{z|y}[\log\{f_i(y_i| z_i,\boldsymbol \theta)\}|\boldsymbol \theta_p, \phi_p] & = \text{E}_{z|y}\left[z_i \log\{f_1(y_i|\boldsymbol \theta_1)\} + (1-z_i) \log\{f_2(y_i|\boldsymbol \theta_2)\} | \boldsymbol \theta_p \phi_p \right] \\
&= \log\{f_1(y_i|\boldsymbol \theta_1)\} \text{E}_{z|y} (z_i|\boldsymbol \theta_p \phi_p) + \log\{f_2(y_i|\boldsymbol \theta_2)\}(1- \text{E}_{z|y}(z_i  | \boldsymbol \theta_p \phi_p )) \\
&= [y_i\log \gamma_i + y_i\log(\lambda_1) - \log y_i! - \gamma_i \lambda_1]\text{E}_{z|y} (z_i|\boldsymbol \theta_p \phi_p) + \\
& \qquad [y_i\log \gamma_i + y_i\log(\lambda_2) - \log y_i! - \gamma_i \lambda_2](1-\text{E}_{z|y} (z_i|\boldsymbol \theta_p \phi_p))
\end{align*}
Similarly, substituting for $g_i$ yields,
\begin{align*}
\text{E}_{z|y}[\log\{g(z_i| \phi)\}|\boldsymbol \theta_p, \phi_p] & = \text{E}_{z|y}(z_i\log(\phi) + (1-z_i)\log(1-\phi)|\boldsymbol \theta_p, \phi_p)\\
&= \log(\phi)\text{E}_{z|y}(z_i|\boldsymbol \theta_p, \phi_p) + \log(1-\phi)(1-\text{E}_{z|y}(z_i|\boldsymbol \theta_p, \phi_p))
\end{align*}
Both of these expressions require only $\text{E}_{z|y}[z_i|\boldsymbol \theta_p, \phi_p]$. From the expression for $k_i$, this expectation can be calculated as
\begin{align*}
\text{E}_{z|y}[z_i|\boldsymbol \theta_p, \phi_p] &= \sum\limits_{z_i \in \Omega_z} \left[ \frac{\left(\phi_p \frac{(\gamma_i \lambda_{p1})^{y_i}}{y_i!}e^{-\gamma_i \lambda_{p1}}\right)^{z_{i}}\left((1-\phi_p) \frac{(\gamma_i \lambda_{p2})^{y_i}}{y_i!}e^{-\gamma_i \lambda_{p2}}\right)^{(1-z_{i})}}{\phi_p \frac{(\gamma_i \lambda_{p1})^{y_i}}{y_i!}e^{-\gamma_i \lambda_{p1}} + (1-\phi_p) \frac{(\gamma_i \lambda_{p2})^{y_i}}{y_i!}e^{-\gamma_i \lambda_{p2}}} z_i \right] \\
&= \frac{\left(\phi_p \frac{(\gamma_i \lambda_{p1})^{y_i}}{y_i!}e^{-\gamma_i \lambda_{p1}}\right)}{\phi_p \frac{(\gamma_i \lambda_{p1})^{y_i}}{y_i!}e^{-\gamma_i \lambda_{p1}} + (1-\phi_p) \frac{(\gamma_i \lambda_{p2})^{y_i}}{y_i!}e^{-\gamma_i \lambda_{p2}}}
\end{align*}
Substitution back into the expression for $Q$ yields:
\begin{align*}
Q(\boldsymbol \theta, \phi|\boldsymbol \theta_p, \phi_p) &= \sum\limits_{i=1}^n\Bigg\{  [ \log(\phi) + y_i\log \gamma_i + y_i\log(\lambda_1) - \log y_i! - \gamma_i \lambda_1]\text{E}_{z|y} (z_i|\boldsymbol \theta_p \phi_p) + \\
& \qquad [\log(1-\phi)+ y_i\log \gamma_i + y_i\log(\lambda_2) - \log y_i! - \gamma_i \lambda_2](1-\text{E}_{z|y} (z_i|\boldsymbol \theta_p \phi_p))] \Bigg\}
\end{align*}
Computation of this expression is the E-step of one iteration in our EM algorithm. The M-step is the maximization of this expression of $Q$ in $\boldsymbol \theta$ and $\phi$. 
\begin{align*}
\frac{\partial Q}{\partial \phi} &= \sum\limits_{i=1}^n \left\{\frac{1}{\phi}\text{E}_{z|y}(z_i|\boldsymbol \theta_p, \phi_p) - \frac{1}{1-\phi}(1-\text{E}_{z|y}(z_i|\boldsymbol \theta_p, \phi_p))\right\}
\end{align*}
so that $\hat{\phi}$ satisfies
\begin{align*}
\hat{\phi} = \frac{1}{n}\sum\limits_{i=1}^n \text{E}_{z|y}(z_i|\boldsymbol \theta_p, \phi_p).
\end{align*}
Therefore, at a given iteration of the EM algorithm, the estimated values of $\phi$ are the average probabilities, under the current values $\boldsymbol \theta_p$, that observations are from steel type A. We let
\begin{align*}
\hat{p}(1|y_i, \boldsymbol \theta_p, \phi_p) = \text{E}_{z|y}(z_i|\boldsymbol \theta_p, \phi_p)
\end{align*}
be the estimated probability that observations $y_i$ belongs to steel type A under the current estimates $\boldsymbol \theta_p, \phi_p$ and then
\begin{align*}
\hat{\phi} = \frac{1}{n} \sum\limits_{i=1}^n\hat{p}(1|y_i, \boldsymbol \theta_p, \phi_p).
\end{align*}
For maximization of $Q$ in $\boldsymbol \theta$,
\begin{align*}
\frac{\partial Q}{\partial \lambda_1} &= \sum\limits_{i=1}^n \left\{\frac{y_i}{\lambda_1} - \gamma_i \right\}\text{E}_{z|y}(z_i|\boldsymbol \theta_p, \phi_p) \\
\frac{\partial Q}{\partial \lambda_2} &= \sum\limits_{i=1}^n \left\{\frac{y_i}{\lambda_2} - \gamma_i \right\}(1-\text{E}_{z|y}(z_i|\boldsymbol \theta_p, \phi_p))
\end{align*}
Thus, we have
\begin{align*}
\hat{\lambda}_1 &= \frac{\sum\limits_{i=1}^n y_i \hat{p}(1|y_i, \boldsymbol \theta_p, \phi_p)}{\sum\limits_{i=1}^n \gamma_i \hat{p}(1|y_i, \boldsymbol \theta_p, \phi_p)} \\
\hat{\lambda}_2 &= \frac{\sum\limits_{i=1}^n y_i (1-\hat{p}(1|y_i, \boldsymbol \theta_p, \phi_p))}{\sum\limits_{i=1}^n \gamma_i (1-\hat{p}(1|y_i, \boldsymbol \theta_p, \phi_p))}
\end{align*}
The basic EM algorithm for this application then consists of the following steps:
\begin{enumerate}
\item Choose starting values $\lambda_1^{(0)}, \lambda_2^{(0)}, \phi^{(0)}$.
\item At iteration $k + 1 = 1,2,\dots$
\begin{enumerate}
\item Compute probabilities that each observation belongs to steel type A
$$ \hat{p}(1|y_i, \phi^{(k)}, \lambda_1^{(k)}, \lambda_2^{(k)}) = \frac{\left(\phi^{(k)} \frac{(\gamma_i \lambda_1^{(k)})^{y_i}}{y_i!}e^{-\gamma_i \lambda_1^{(k)}}\right)}{\phi^{(k)} \frac{(\gamma_i \lambda_1^{(k)})^{y_i}}{y_i!}e^{-\gamma_i \lambda_1^{(k)}} + (1-\phi^{(k)}) \frac{(\gamma_i \lambda_2^{(k)})^{y_i}}{y_i!}e^{-\gamma_i \lambda_2^{(k)}}} $$
\item Update the value of $\phi^{(k)}$ to 
$$ \phi^{(k+1)} = \frac{1}{n}\sum_{i=1}^n \hat{p}(1|y_i, \phi^{(k)}, \lambda_1^{(k)})$$
\item Update $\lambda_1^{(k)}$ and $\lambda_2^{(k)}$ to $\lambda_1^{(k+1)}$ and $\lambda_2^{(k+1)}$ as follows
\begin{align*}
\lambda_1^{(k+1)} &= \frac{\sum\limits_{i=1}^n y_i \hat{p}(1|y_i, y_i, \phi^{(k)}, \lambda_1^{(k)})}{\sum\limits_{i=1}^n \gamma_i \hat{p}(1|y_i, y_i, \phi^{(k)}, \lambda_1^{(k)})} \\
\lambda_2^{(k+1)} &= \frac{\sum\limits_{i=1}^n y_i (1-\hat{p}(1|y_i, y_i, \phi^{(k)}, \lambda_1^{(k)}))}{\sum\limits_{i=1}^n \gamma_i (1-\hat{p}(1|y_i, y_i, \phi^{(k)}, \lambda_1^{(k)}))}
\end{align*}
\end{enumerate}
\item Iterate step 2 until $|\lambda_1^{(k+1)} - \lambda_1^{(k)}| \le \delta$, $|\lambda_2^{(k+1)} - \lambda_2^{(k)}| \le \delta$, and $|\phi^{(k+1)} - \phi^{(k)}| \le \delta$.
\end{enumerate}

<<est_hetero, echo=FALSE, results='hide'>>=
ests<-em_algorithm(.5, 0.01, 5, y=subway.dat$flaws, n=subway.dat$length)

subway.dat$probs <- phat_fun(ests[1], ests[2], ests[3], y=subway.dat$flaws, n=subway.dat$length)

h<-hessian(ests[1], ests[2], ests[3], y=subway.dat$flaws, n=subway.dat$length)
lb<-ests[1:3] - qnorm(.975)*sqrt(diag(solve(h)))
ub<-ests[1:3] + qnorm(.975)*sqrt(diag(solve(h)))
inf_print <- paste(apply(solve(h),1,function(x){paste(paste(round(x,4), collapse="&"),"\\\\",sep="")}),collapse=" ")
@

A plethora of starting values for the algorithm were explored and all resulted in the same maximum likelihood estimates of $\lambda_1$, $\lambda_2$, and $\phi$, shown in Table ~\ref{tab:hetero} with a convergence criterion of $\delta = 10^{-8}$. The algorithm converged very quickly. For example, with starting values of $\phi^{(0)} = .5, \lambda_1^{(0)} = 0.01, \lambda_2^{(0)} = 5$, the algorithm converged in \Sexpr{ests['n.iter']} iterations. To assess variation in these estimates, we will consider both Wald theory and parametric bootstrap confidence intervals.

\subsection{95\% Wald Theory Intervals}
To compute standard errors for the Wald intervals, we will first derive the observed information matrix corresponding to the marginal model $h(y_i|\gamma_1,\lambda_1,\lambda_2,\phi)$. Then from expression of the log likelihood as $l(\lambda_1,\lambda_2,\phi) = \sum l_i(\lambda_1,\lambda_2,\phi) \sum \log\{h(y_i|\gamma_1,\boldsymbol \theta)\}$ for $\boldsymbol \theta=(\lambda_1,\lambda_2,\phi)$,

\begin{align*}
\frac{\partial l(\boldsymbol \theta)}{\partial \theta_j} &= \sum\limits_{i=1}^n \frac{\partial l_i(\boldsymbol \theta)}{\theta_j}\\
\frac{\partial^2 l(\boldsymbol \theta)}{\partial \theta_j\partial \theta_k} &= \sum\limits_{i=1}^n \frac{\partial^2 l_i(\boldsymbol \theta)}{\partial \theta_j \partial \theta_k}\\ 
\end{align*}
\begin{align*}
\frac{\partial l_i(\boldsymbol \theta)}{\theta_j} &= \frac{\frac{\partial h(y_i|\boldsymbol \theta)}{\partial \theta_j}}{h(y_i|\boldsymbol \theta)} \\
\frac{\partial^2 l(\boldsymbol \theta)}{\partial \theta_j\partial \theta_k} &= \frac{\frac{\partial^2 h(y_i|\boldsymbol \theta)}{\partial \theta_j \partial \theta_k}}{h(y_i|\boldsymbol \theta)} - \frac{\frac{\partial h(y_i|\boldsymbol \theta)}{\partial \theta_j}\frac{\partial h(y_i|\boldsymbol \theta)}{\partial \theta_k}}{\{h(y_i|\boldsymbol \theta)\}^2}
\end{align*}
Where
\begin{align*}
\frac{\partial h(y_i|\gamma_i, \lambda_1,\lambda_2,\phi)}{\partial \lambda_1} &= \phi \frac{\partial f_1(y_i|\gamma_i,\lambda_1)}{\partial \lambda_1}\\
\frac{\partial h(y_i|\gamma_i, \lambda_1,\lambda_2,\phi)}{\partial \lambda_2} &= (1-\phi) \frac{\partial f_2(y_i|\gamma_i,\lambda_2)}{\partial \lambda_2}\\
\frac{\partial h(y_i|\gamma_i, \lambda_1,\lambda_2,\phi)}{\partial \phi} &= f_1(y_i|\gamma_i,\lambda_1) - f_2(y_i|\gamma_i,\lambda_2)\\
\frac{\partial^2 h(y_i|\gamma_i, \lambda_1,\lambda_2,\phi)}{\partial \lambda_1^2} &= \phi \frac{\partial^2 f_1(y_i|\gamma_i,\lambda_1)}{\partial \lambda_1^2}\\
\frac{\partial^2 h(y_i|\gamma_i, \lambda_1,\lambda_2,\phi)}{\partial \lambda_2^2} &= (1-\phi) \frac{\partial^2 f_2(y_i|\gamma_i,\lambda_2)}{\partial \lambda_2^2}\\
\frac{\partial^2 h(y_i|\gamma_i, \lambda_1,\lambda_2,\phi)}{\partial \phi^2} &= 0\\
\frac{\partial^2 h(y_i|\gamma_i, \lambda_1,\lambda_2,\phi)}{\partial \lambda_1 \partial \lambda_2} &= 0\\
\frac{\partial^2 h(y_i|\gamma_i, \lambda_1,\lambda_2,\phi)}{\partial \lambda_1 \partial \phi} &= \frac{\partial f_1(y_i|\gamma_i,\lambda_1)}{\partial \lambda_1}\\
\frac{\partial^2 h(y_i|\gamma_i, \lambda_1,\lambda_2,\phi)}{\partial \lambda_2 \partial \phi} &= -\frac{\partial f_2(y_i|\gamma_i,\lambda_2)}{\partial \lambda_2}
\end{align*}
And finally, for $\lambda_j; j=1,2$
\begin{align*}
\frac{\partial f_j(y_i|\gamma_i,\lambda_j)}{\partial \lambda_j} &= f_j(y_i|\gamma_i, \lambda_j)\left(\frac{y_i}{\lambda_j} - \gamma_i\right)\\
 \frac{\partial^2 f_j(y_i|\gamma_i,\lambda_j)}{\partial \lambda_j^2} &= f_j(y_i|\gamma_i, \lambda_j)\left(\frac{y_i}{\lambda_j} - \gamma_i\right)^2 - f_j(y_i|\gamma_i,\lambda_j)\left(\frac{y_i}{\lambda_j^2}\right)
\end{align*}
The observed information matrix can thus be computed by plugging in the estimates of $\lambda_1,\lambda_2,\phi$ from the EM algorithm to these quantities.

Then, the inverse observed information matrix used to compute Wald interval estimates, shown in Table~\ref{tab:hetero}, is
\begin{align*}
\mathcal{I}^{-1} = 
\left(\begin{matrix} 
\Sexpr{inf_print}
\end{matrix}\right)
\end{align*}

\newpage

\subsection{Parametric Bootstrap Intervals}
95\% bootstrap intervals were constructed for $\lambda_1$, $\lambda_2$ and $\phi$ using the following methodology.

For $m= 1, \dots, M$,
\begin{enumerate}
\item Simulate $\mathbf{y}^{(m)} = (y^{(m)}_1, \dots, y^{(m)}_n)$ from $h(y|\hat{\lambda}_1, \hat{\lambda}_2, \hat{\phi})$.
  \begin{enumerate}
  \item Simulate $\mathbf{u}^{(m)} = (u^{(m)}_1, \dots, u^{(m)}_n)$ from a Unif$(0,1)$.
  \item If $u^{(m)}_i < \hat{\phi}$, simulate $y^{(m)}_i$ from Pois$(\hat{\lambda}_1)$.
  \item If $u^{(m)}_i \geq \hat{\phi}$, simulate $y^{(m)}_i$ from Pois$(\hat{\lambda}_2)$.
  \end{enumerate}
\item Estimate $\hat{\theta}^{(m)} = (\hat{\lambda}^{(m)}_1$, $\hat{\lambda}^{(m)}_2$, $\hat{\phi}^{(m)}$) using the EM algorithm with $\mathbf{y}^{(m)}$
\end{enumerate}

Then 95\% interval estimates for elements of $\theta$ are $\left(2\hat{\theta} - \hat{\theta}^{m}_{\lbrack (M+1)(0.975) \rbrack}, 2\hat{\theta} - \hat{\theta}^{m}_{\lbrack (M+1)(0.025) \rbrack}\right)$. These intervals are shown in Table~\ref{tab:hetero}.

<<boot_hetero, echo=FALSE, results='asis'>>=
n<-subway.dat$length
ests.boot<-NULL
for(j in 1:1000){
  phis<-runif(75)
  y<-1:75
  for(i in 1:75){
    if(phis[i] < ests[1]){
      y[i]<-rpois(1, ests[2]*n[i])
    }else{
      y[i]<-rpois(1, ests[3]*n[i])
    }
  }
  ests.boot<-rbind(ests.boot, c(em_algorithm(.5, .01, 5, y=y, n=subway.dat$length)[1:3]))
}
ests.boot<-rbind(ests[1:3], ests.boot)
boot_intervals<-t(apply(ests.boot, 2, function(u) 2*u[1] - sort(u[-1])[c(975, 25)]))
@

<<tab_hetero, echo=FALSE, results='asis'>>=
res <- data.frame(Estimate=ests[1:3], Interval1=paste("(", round(lb,4), ", ", round(ub,4),")", sep=""), Interval2=paste("(", round(boot_intervals[,1],4), ", ", round(boot_intervals[,2],4),")", sep=""))
names(res)[2] <- "95 \\% Wald Interval"
names(res)[3] <- "95 \\% Bootstrap Interval"
rownames(res) <- c("$\\phi$","$\\lambda_1$","$\\lambda_2$")
print(xtable(res, caption="Estimates from EM algorithm and 95\\% Wald and Bootstrap intervals for parameters of the two component poisson distributions.", label="tab:hetero", align=c("r", "r", "r", "r")), sanitize.text.function=function(x){x})
@

% <<tab_boot, echo=FALSE, results='asis'>>=
% res.boot <- data.frame(Estimate=ests[1:3], Interval=paste("(", round(boot_intervals[,1],4), ", ", round(boot_intervals[,2],4),")", sep=""))
% names(res.boot)[2] <- "95 \\% Bootstrap Interval"
% rownames(res.boot) <- c("$\\phi$","$\\lambda_1$","$\\lambda_2$")
% print(xtable(res.boot, caption="Original estimates from the EM algorithm and 95\\% bootstrap intervals for parameters of the two component poisson distributions.", label="tab:boot"), sanitize.text.function=function(x){x})
% @

Intervals using Wald theory and parametric bootstrap for the parameters are consistent with each other. The estimates and intervals for $\lambda_1$ and $\lambda_2$, shown in Table~\ref{tab:hetero}, indicate that the number of flaws per km for type A is significantly lower than the number of flaws per km for type B. 

\subsection{Model Assessment}

<<chi_hetero, echo=FALSE>>=
phi.est<-ests[1]
l1<-ests[2]
l2<-ests[3]
subway.dat$exp.counts2<-with(subway.dat, probs*l1*subway.dat$length + (1-probs)*l2*subway.dat$length)
subway.dat$chi_sq2 <- with(subway.dat, (flaws-exp.counts2)^2/exp.counts2)
chi.test2 <- sum(subway.dat$chi_sq2)
@

Using the parameter estimates from Table~\ref{tab:hetero} and the estimated probabilities a segment was built with steel type A, expected flaws were computed for each segment of track. These expected counts are reported in Table~\ref{tab:hetero2}, along with individual contributions to the overall Chi-square test statistic of \Sexpr{chi.test2} with, which with \Sexpr{nrow(subway.dat) - 1} degress of freedom, has an associated $p$-value of \Sexpr{1-pchisq(chi.test2, nrow(subway.dat)-1)}. This $p$-value indicates that the heterogeneous model results in a substantial improvement over the homogeneous model, discussed previously, in estimation of the number of flaws in subway track segments.

\newpage

{\footnotesize
<<tab_hetero2, echo=FALSE, results='asis', warning=FALSE>>=
res <- subway.dat[,c(1,2,3,7,8,6)]
names(res) <- c("Segment","Length", "Observed", "Expected", "$\\chi^2_i$", "Pr(A)")
res <- res[order(res[,3], -res[,4]),]
left <- res[1:ceiling(nrow(res)/2),]
right <- res[(ceiling(nrow(res)/2)+1):nrow(res),]

print(xtable(cbindPad(left, right), label='tab:hetero2', caption="Observed and expected flaws in subway track section, along with contributions to the Chi-square test statistic for the heterogeneous flaw rates model.", digits=c(0,0,0,0,4,4,4,0,0,0,4,4,4), align=c("c","c","c","c","c","c","c","|","c","c","c","c","c","c")),sanitize.text.function=function(x){x},tabular.environment='longtable', floating=FALSE, include.rownames=FALSE)
@
}
Also reported in Table~\ref{tab:hetero2} are estimated probabilities that each track segment was built with steel type A. While the overall estimate of this probability is $\hat{\phi} = \Sexpr{round(ests["phi"],4)}$, the segment values indicate those segments with fewer flaws were much more likely to be built from steel type A, than those with more flaws. This can also be seen in Figure~\ref{fig:plot_hetero}, and is consistent with the assumption that steel type A is less susceptible to flaws than steel type B.

<<plot_hetero, echo=FALSE, fig.height=4, fig.width=6, out.width='.55\\textwidth', fig.cap='Probabilities of each track segment being built from steel type A compared to the number of flaws in each segment.', fig.show='hold', message=FALSE, fig.pos='ht'>>=
qplot(x=flaws, y=probs, data=subway.dat, xlab="Number of Flaws", ylab="Probability of Steel Type A")
@

<<plot_hetero2, echo=FALSE, fig.height=4, fig.width=6, out.width='.8\\textwidth', fig.cap='Comparison of expected versus observed flaw counts for each track segment from both the homoegeneous and the hetergeneous models.', fig.show='hold', message=FALSE, fig.pos='H'>>=
subway.dat$segment.f <- factor(subway.dat$segment, levels=subway.dat$segment[order(-subway.dat$flaws)])

subway.dat.m <- melt(subway.dat[,c(9, 3, 4, 7)], id.vars=c("segment.f"))

ggplot(data=subway.dat.m) +
  geom_linerange(aes(x=segment.f, ymin=0, ymax=value), data=subset(subway.dat.m, variable == "flaws")) +
  geom_point(aes(x=segment.f, y=value, group=variable, colour=variable, pch=variable)) +
  scale_colour_manual(name="Flaws", values=c("black", "red", "blue"), labels=c("Observed", "Expected Homogeneous", "Expected Heterogeneous")) +
  scale_shape_manual(name="Flaws", values=c(16,15,17), labels=c("Observed", "Expected Homogeneous", "Expected Heterogeneous")) +
  xlab("Track Segment") + ylab("Number of Flaws") + theme(legend.position="bottom") +
  scale_x_discrete(breaks=NULL)
  
@

Figure~\ref{fig:plot_hetero2} shows the observed number of flaws against the expected number of flaws from the homogeneous and heterogeneous models. The heterogeneous model appears to do a much better job fitting segments with large and small observed flaws than the homogeneous model. 